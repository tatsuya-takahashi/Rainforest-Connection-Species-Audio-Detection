{"format": "torch", "nodes": [{"name": "conv", "id": 2976367759168, "class_name": "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))", "parameters": [["weight", [1, 1, 3, 3]], ["bias", [1]]], "output_shape": [[40, 1, 2811, 126]], "num_parameters": [9, 1]}, {"name": "linear", "id": 2976367759120, "class_name": "Linear(in_features=63, out_features=128, bias=True)", "parameters": [["weight", [128, 63]], ["bias", [128]]], "output_shape": [[40, 1, 1405, 128]], "num_parameters": [8064, 128]}, {"name": "conformerblock", "id": 2976367759216, "class_name": "ConformerBlock(\n  (ff1): Scale(\n    (fn): PreNorm(\n      (fn): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=128, out_features=512, bias=True)\n          (1): Swish()\n          (2): Dropout(p=0.35, inplace=False)\n          (3): Linear(in_features=512, out_features=128, bias=True)\n          (4): Dropout(p=0.35, inplace=False)\n        )\n      )\n      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (attn): PreNorm(\n    (fn): Attention(\n      (to_q): Linear(in_features=128, out_features=512, bias=False)\n      (to_kv): Linear(in_features=128, out_features=1024, bias=False)\n      (to_out): Linear(in_features=512, out_features=128, bias=True)\n      (rel_pos_emb): Embedding(1025, 64)\n      (dropout): Dropout(p=0.35, inplace=False)\n    )\n    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n  (conv): ConformerConvModule(\n    (net): Sequential(\n      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (1): Rearrange('b n c -> b c n')\n      (2): Conv1d(128, 512, kernel_size=(1,), stride=(1,))\n      (3): GLU()\n      (4): DepthWiseConv1d(\n        (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n      )\n      (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Swish()\n      (7): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n      (8): Rearrange('b c n -> b n c')\n      (9): Dropout(p=0.35, inplace=False)\n    )\n  )\n  (ff2): Scale(\n    (fn): PreNorm(\n      (fn): FeedForward(\n        (net): Sequential(\n          (0): Linear(in_features=128, out_features=512, bias=True)\n          (1): Swish()\n          (2): Dropout(p=0.35, inplace=False)\n          (3): Linear(in_features=512, out_features=128, bias=True)\n          (4): Dropout(p=0.35, inplace=False)\n        )\n      )\n      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (post_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n)", "parameters": [["ff1.fn.fn.net.0.weight", [512, 128]], ["ff1.fn.fn.net.0.bias", [512]], ["ff1.fn.fn.net.3.weight", [128, 512]], ["ff1.fn.fn.net.3.bias", [128]], ["ff1.fn.norm.weight", [128]], ["ff1.fn.norm.bias", [128]], ["attn.fn.to_q.weight", [512, 128]], ["attn.fn.to_kv.weight", [1024, 128]], ["attn.fn.to_out.weight", [128, 512]], ["attn.fn.to_out.bias", [128]], ["attn.fn.rel_pos_emb.weight", [1025, 64]], ["attn.norm.weight", [128]], ["attn.norm.bias", [128]], ["conv.net.0.weight", [128]], ["conv.net.0.bias", [128]], ["conv.net.2.weight", [512, 128, 1]], ["conv.net.2.bias", [512]], ["conv.net.4.conv.weight", [256, 1, 31]], ["conv.net.4.conv.bias", [256]], ["conv.net.5.weight", [256]], ["conv.net.5.bias", [256]], ["conv.net.7.weight", [128, 256, 1]], ["conv.net.7.bias", [128]], ["ff2.fn.fn.net.0.weight", [512, 128]], ["ff2.fn.fn.net.0.bias", [512]], ["ff2.fn.fn.net.3.weight", [128, 512]], ["ff2.fn.fn.net.3.bias", [128]], ["ff2.fn.norm.weight", [128]], ["ff2.fn.norm.bias", [128]], ["post_norm.weight", [128]], ["post_norm.bias", [128]]], "output_shape": [[40, 1405, 128]], "num_parameters": [65536, 512, 65536, 128, 128, 128, 65536, 131072, 65536, 128, 65600, 128, 128, 128, 128, 65536, 512, 7936, 256, 256, 256, 32768, 128, 65536, 512, 65536, 128, 128, 128, 128, 128]}, {"name": "decoder", "id": 2976367917088, "class_name": "Linear(in_features=179840, out_features=24, bias=True)", "parameters": [["weight", [24, 179840]], ["bias", [24]]], "output_shape": [[40, 24]], "num_parameters": [4316160, 24]}], "edges": []}