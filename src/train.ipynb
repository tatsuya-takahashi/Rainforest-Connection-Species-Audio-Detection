{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Based on clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expname:exp_28_fullMultiLabel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT = \"RFCX\"\n",
    "EXP_NUM = \"28\"\n",
    "EXP_TITLE = \"multilabel02\"\n",
    "EXP_NAME = \"exp_\" + EXP_NUM + \"_\" + EXP_TITLE\n",
    "IS_WRITRE_LOG = True\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'train_clip'\n",
    "print('expname:' + EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import os\n",
    "# import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import librosa\n",
    "import wandb\n",
    "from time import sleep\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "import torch_optimizer as toptim\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.utils.data as torchdata\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional\n",
    "from numpy.random import beta\n",
    "from pathlib import Path\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from torchviz import make_dot\n",
    "from conformer import ConformerConvModule\n",
    "from conformer import ConformerBlock\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "import librosa.display\n",
    "from resnest.torch import resnest50\n",
    "from skimage.filters import gaussian\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import gaussian\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import exposure, util\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.scroll_box { height:90em  !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight\n",
    "# model_efn = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "# model_efn = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "# model_efn = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "# model_efn.to(device); # calculate on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(931839,)\n(320, 938)\n938 320\n"
     ]
    }
   ],
   "source": [
    "# 5get length\n",
    "class params:\n",
    "    sr = 48000\n",
    "    n_mels = 320\n",
    "    fmin = 40\n",
    "    fmax = sr // 2\n",
    "    fft = 2048\n",
    "    hop = 512\n",
    "    clip_frame = 10 * 48000\n",
    "    augnum = 100\n",
    "\n",
    "def wav2mel(wavnp):\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        wavnp, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax, n_fft=params.fft, hop_length=params.hop, \n",
    "    )\n",
    "    melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "    # # normalize\n",
    "    # melspec = melspec - np.min(melspec)\n",
    "    # melspec = melspec / np.max(melspec)\n",
    "\n",
    "    eps=1e-6 # avoid  divided by 0\n",
    "    mean = melspec.mean()\n",
    "    std = melspec.std()\n",
    "    spec_norm = (melspec - mean) / (std + eps)\n",
    "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
    "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
    "    spec_scaled = spec_scaled.astype(np.uint8)\n",
    "    spec_scaled = np.asarray(spec_scaled)\n",
    "\n",
    "    return spec_scaled\n",
    "\n",
    "wavnp = np.load(Path('../input//rfcx-species-audio-detection/train_mel/0.npy'))\n",
    "print(wavnp.shape)\n",
    "sample = wav2mel(wavnp[0: 10 * params.sr]) # 10s clipping\n",
    "\n",
    "# sample data\n",
    "# sample = torch.load(Path(\"e:/002_datasets/000_RFCX/train_mel_clip_aug/0_0.pt\"))\n",
    "# sample = torch.from_numpy(np.load(Path(\"../input/rfcx-species-audio-detection/train_mel/0.npy\")))\n",
    "# sample = torch.load(Path(\"../input/rfcx-species-audio-detection/train_mel_clip_aug/0_0.pt\"))\n",
    "\n",
    "# channel, seq, dim\n",
    "print(sample.shape)\n",
    "# print(sample[np.newaxis, :, :].shape)\n",
    "clip_len = int(sample.shape[1])\n",
    "clip_dim = int(sample.shape[0])\n",
    "print(clip_len, clip_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # expeliment\n",
    "# clip = sample.T\n",
    "# print(\"clip\", clip.shape)\n",
    "\n",
    "# # stacking\n",
    "# img = torch.from_numpy(np.array([\n",
    "#         [clip],[clip],[clip]\n",
    "#     ])).float().transpose(0, 1)\n",
    "# print(\"img\", img.shape)\n",
    "\n",
    "# # encoding\n",
    "# enc = model_efn.extract_features(img.to(device))\n",
    "# print(\"enc\", enc.shape)\n",
    "\n",
    "# enc = enc.detach().cpu()\n",
    "\n",
    "# # save\n",
    "# ch = enc.shape[1]\n",
    "# enc_len = enc.shape[2]\n",
    "# enc_dim = enc.shape[3]\n",
    "# print('ch, enc_len, enc_dim', ch, enc_len, enc_dim)\n",
    "\n",
    "# del enc\n",
    "\n",
    "ch = 1792\n",
    "enc_len = 30\n",
    "enc_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    resnet_model = resnest50(pretrained=True)\n",
    "    num_ftrs = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(num_ftrs, config.NUM_BIRDS)\n",
    "    # resnet_model = resnet_model.to(device)\n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dict2(dict): \n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__(*args, **kwargs) \n",
    "        self.__dict__ = self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict2({\n",
    "    \"fft\":                2048,\n",
    "    \"hop\":                512,\n",
    "    \"sr\":                 48000,\n",
    "    \"mel\":                320,\n",
    "    \"SEED\":               42,\n",
    "    # \"INPUT\":              Path(\"../input/rfcx-species-audio-detection/train\"),\n",
    "    # \"TRAIN_AUDIO_ROOT\":   Path(\"../input/rfcx-species-audio-detection/train_mel_clip_aug/\"),\n",
    "    # \"TEST_AUDIO_ROOT\":    Path\"../input/rfcx-species-audio-detection/train_mel_clip_aug/0_0.pt\"),\n",
    "    # \"TRAIN_TP\":           Path(\"../input/rfcx-species-audio-detection/train_tp.csv\"),\n",
    "    # \"TRAIN_TP_MEL\":       Path(\"../input/rfcx-species-audio-detection/train_tp_mel.csv\"),\n",
    "    # \"SUB\":                Path(\"../input/rfcx-species-audio-detection/sample_submission.csv\"),\n",
    "    \"TEST_AUDIO_FLAC\":    Path(\"../input/rfcx-species-audio-detection/test\"),\n",
    "    \"TRAIN_AUDIO_ROOT\":   Path(\"e:/002_datasets/000_RFCX/train_mel_clip_aug/\"),\n",
    "    \"TEST_AUDIO_ROOT\":    Path(\"../input/rfcx-species-audio-detection/test_mel\"),\n",
    "    \"VALID_AUDIO_ROOT\":   Path(\"e:/002_datasets/000_RFCX/valid_mel_clip/\"),\n",
    "    \"TRAIN_TP\":           Path(\"../input/rfcx-species-audio-detection/train_tp.csv\"),\n",
    "    \"TRAIN_TP_CSV\":       Path(\"../input/rfcx-species-audio-detection/train_tp_mel.csv\"),\n",
    "    \"VALID_CSV\":          Path(\"../input/rfcx-species-audio-detection/valid.csv\"),\n",
    "    \"TEST_CSV\":           Path(\"../input/rfcx-species-audio-detection/test.csv\"),\n",
    "    \"SUB\":                Path(\"../input/rfcx-species-audio-detection/sample_submission.csv\"),\n",
    "    # \"DIM\":                sample.shape[0],\n",
    "    # \"SEQ_LEN\":            int(sample.shape[1] * 0.8),\n",
    "    # \"DIM\":                dim,\n",
    "    # \"ENC_LEN\":            seq_len,\n",
    "    \"MIX_LABEL\":          1.0,\n",
    "    \"CLIP_LEN\":           clip_len,\n",
    "    \"CLIP_DIM\":           clip_dim,\n",
    "    \"ENC_CH\":             ch,\n",
    "    \"ENC_LEN\":            enc_len,\n",
    "    \"ENC_DIM\":            enc_dim,\n",
    "    \"KERNEL_SIZE\":        3,\n",
    "    \"KERNEL_STRIDE\":      1,\n",
    "    \"KERNEL_SIZE_SEQ\":    3,\n",
    "    \"POOL_SIZE\":          2,\n",
    "    \"POOL_STRIDE\":        2,\n",
    "    \"NUM_BIRDS\":          24,\n",
    "    \"N_FOLDS\":            5,\n",
    "    \"BATCH_NUM\":          20,\n",
    "    \"VALID_BATCH_NUM\":    20,\n",
    "    \"EPOCH_NUM\":          30,\n",
    "    \"DROPOUT\":            0.35,\n",
    "    \"lr\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"gamma\": 0.7,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 0,\n",
    "    \"t_max\":              10,\n",
    "    \"TEST_SIZE\":          0.2,\n",
    "    \"MIXUP\":              0.0,\n",
    "    \"MIXUP_PROB\":         -1.0,\n",
    "    \"SPEC_PROB\":          -1,\n",
    "    \"spec_time_w\":        0,\n",
    "    \"spec_time_stripes\":  0,\n",
    "    \"spec_freq_w\":        0,\n",
    "    \"spec_freq_stripes\":  0,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "set_seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_augmenter = SpecAugmentation(time_drop_width=config.spec_time_w, time_stripes_num=config.spec_time_stripes, \n",
    "            freq_drop_width=config.spec_freq_w, freq_stripes_num=config.spec_freq_stripes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img.shape)\n",
    "# print(spec_augmenter(img).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stacking\n",
    "# img = torch.from_numpy(np.array([\n",
    "#         [clip],[clip],[clip]\n",
    "#     ])).float().transpose(0, 1)\n",
    "\n",
    "# print(img.shape)\n",
    "\n",
    "\n",
    "# s_img = spec_augmenter(img)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# figure1ch = librosa.display.specshow(\n",
    "#     s_img.numpy()[0][0].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# figure2ch = librosa.display.specshow(\n",
    "#     s_img.numpy()[0][1].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# figure3ch = librosa.display.specshow(\n",
    "#     s_img.numpy()[0][2].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "\n",
    "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0.:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    # lam = max(lam, 1 - lam)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index,:]\n",
    "    # mixed_y = lam * y + (1 - lam) * y[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    # return mixed_x, mixed_y\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# def mixup_criterion(y_a, y_b, lam):\n",
    "#     return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class OneOf:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        n_trns = len(self.transforms)\n",
    "        trns_idx = np.random.choice(n_trns)\n",
    "        trns = self.transforms[trns_idx]\n",
    "        return trns(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorednoise as cn\n",
    "\n",
    "class PinkNoiseSNR(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5.0, max_snr=20.0, **kwargs):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise ** 2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchShift(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_steps=5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        n_steps = np.random.randint(-self.max_steps, self.max_steps)\n",
    "        augmented = librosa.effects.pitch_shift(y, sr=self.sr, n_steps=n_steps)\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumeControl(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, db_limit=10, mode=\"uniform\"):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        assert mode in [\"uniform\", \"fade\", \"fade\", \"cosine\", \"sine\"], \\\n",
    "            \"`mode` must be one of 'uniform', 'fade', 'cosine', 'sine'\"\n",
    "\n",
    "        self.db_limit= db_limit\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.db_limit, self.db_limit)\n",
    "        if self.mode == \"uniform\":\n",
    "            db_translated = 10 ** (db / 20)\n",
    "        elif self.mode == \"fade\":\n",
    "            lin = np.arange(len(y))[::-1] / (len(y) - 1)\n",
    "            db_translated = 10 ** (db * lin / 20)\n",
    "        elif self.mode == \"cosine\":\n",
    "            cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * cosine / 20)\n",
    "        else:\n",
    "            sine = np.sin(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "            db_translated = 10 ** (db * sine / 20)\n",
    "        augmented = y * db_translated\n",
    "        return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_flip(img):\n",
    "    horizontal_flip_img = img[:, ::-1]\n",
    "    return horizontal_flip_img\n",
    "\n",
    "def vertical_flip(img):\n",
    "    vertical_flip_img = img[::-1, :]\n",
    "    return vertical_flip_img\n",
    "\n",
    "def addNoisy(img):\n",
    "    noise_img = util.random_noise(img)\n",
    "    return noise_img\n",
    "\n",
    "def contrast_stretching(img):\n",
    "    contrast_img = exposure.rescale_intensity(img)\n",
    "    return contrast_img\n",
    "\n",
    "def randomGaussian(img):\n",
    "    gaussian_img = gaussian(img)\n",
    "    return gaussian_img\n",
    "\n",
    "def grayScale(img):\n",
    "    gray_img = rgb2gray(img)\n",
    "    return gray_img\n",
    "\n",
    "def randomGamma(img):\n",
    "    img_gamma = exposure.adjust_gamma(img)\n",
    "    return img_gamma\n",
    "\n",
    "def nonAug(img):\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_transform = transforms.Compose([\n",
    "  PinkNoiseSNR(min_snr=10, always_apply=False, p=0.5),\n",
    "  # PitchShift(max_steps=2, sr=params.sr, always_apply=False, p=0.3),\n",
    "  # VolumeControl(mode=\"sine\", always_apply=False, p=0.3)\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    # transforms.CenterCrop((config.mel, config.CLIP_LEN)),\n",
    "    # transforms.ToTensor()\n",
    "])\n",
    "label_transform = transforms.Compose([\n",
    "    # transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[8, 21]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Data load\n",
    "df_train_tp = pd.read_csv(config.TRAIN_TP_CSV)\n",
    "\n",
    "# create dictionary\n",
    "dic_rec_spec = {}\n",
    "for index, row in df_train_tp.iterrows():\n",
    "    if row[\"recording_id\"] not in dic_rec_spec:\n",
    "        dic_rec_spec[row[\"recording_id\"]] = [row[\"species_id\"]]\n",
    "    else:\n",
    "        dic_rec_spec[row[\"recording_id\"]].append(row[\"species_id\"])\n",
    "dic_rec_spec[\"77299bde7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1216\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      id  species_id recording_id    t_min   t_maxs      offs    0    1    2  \\\n584  584           8    77299bde7   5.7227   9.8453  274689.6  0.0  0.0  0.0   \n585  585          21    77299bde7  42.3787  43.4720  427521.6  0.0  0.0  0.0   \n\n       3    4    5    6    7    8    9   10   11   12   13   14   15   16  \\\n584  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n585  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n      17   18   19   20   21   22   23  \n584  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n585  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>species_id</th>\n      <th>recording_id</th>\n      <th>t_min</th>\n      <th>t_maxs</th>\n      <th>offs</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>584</th>\n      <td>584</td>\n      <td>8</td>\n      <td>77299bde7</td>\n      <td>5.7227</td>\n      <td>9.8453</td>\n      <td>274689.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>585</th>\n      <td>585</td>\n      <td>21</td>\n      <td>77299bde7</td>\n      <td>42.3787</td>\n      <td>43.4720</td>\n      <td>427521.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# add column per birds and flogs\n",
    "for col in range(24):\n",
    "    df_train_tp[col] = 0.\n",
    "\n",
    "# one-hot encoding\n",
    "for index, row in df_train_tp.iterrows():\n",
    "    specId = row[\"species_id\"]\n",
    "    df_train_tp.iloc[index, df_train_tp.columns.get_loc(specId)] = 1\n",
    "\n",
    "    for duplicateSpecId in dic_rec_spec[row[\"recording_id\"]]:\n",
    "        if specId != duplicateSpecId:\n",
    "            df_train_tp.iloc[index, df_train_tp.columns.get_loc(duplicateSpecId)] = 1 * config.MIX_LABEL\n",
    "\n",
    "# grouping\n",
    "# df_train_tp = df_train_tp.groupby(\"recording_id\", as_index=False).max()\n",
    "\n",
    "# check\n",
    "print(len(df_train_tp))\n",
    "display(df_train_tp[df_train_tp[\"recording_id\"] == \"77299bde7\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_train_tp[df_train_tp[\"recording_id\"] == \"00b404881\"].head())\n",
    "# df_train_tp_grouped = df_train_tp.groupby([\"species_id\", \"recording_id\"], as_index=False).max()\n",
    "# display(df_train_tp_grouped[df_train_tp_grouped[\"recording_id\"] == \"00b404881\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "id 584\nspecid 8\nrecid 77299bde7\nlabel [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\nlabel shape (24,)\nid len 1216\noffset 274689.6\noffset 5.7227\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ids = []\n",
    "specIds = []\n",
    "record_ids = []\n",
    "labels = []\n",
    "offsets = []\n",
    "for index, row in df_train_tp.iterrows():\n",
    "    ids.append(row.values[0])\n",
    "    specIds.append(row.values[1])\n",
    "    record_ids.append(row.values[2])\n",
    "    labels.append(row.values[6:30])\n",
    "    offsets.append(row.values[5])\n",
    "\n",
    "labels = np.array(labels).astype(float)\n",
    "\n",
    "print('id', ids[584])\n",
    "print('specid', specIds[584])\n",
    "print('recid', record_ids[584])\n",
    "print('label', labels[584])\n",
    "print('label shape', labels[584].shape)\n",
    "print('id len', len(ids))\n",
    "print('offset', offsets[584])\n",
    "print('offset', offsets[584] / params.sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RainforestDatasets(torch.utils.data.Dataset):\n",
    "#     def __init__(self, _transform, train = True):\n",
    "#         self.transform = _transform\n",
    "#         self.train = train\n",
    "\n",
    "#         # data load\n",
    "#         self.labelset = labels\n",
    "#         self.dataset = []\n",
    "#         for id in ids:\n",
    "#             # read npy\n",
    "#             melspec = torch.load(os.path.join(config.TRAIN_AUDIO_ROOT, str(id) + \".pt\")) # (dim, seq_len)\n",
    "#             # melspec = torch.from_numpy(melspec)\n",
    "#             # melspec = melspec.unsqueeze(0) # add channel for first convolution\n",
    "#             # melspec = melspec[np.newaxis, :, :] # add channel for first convolution\n",
    "#             self.dataset.append(melspec)\n",
    "\n",
    "#         self.dataset = np.array(self.dataset).astype(float)\n",
    "#         self.datanum = len(self.dataset)\n",
    "        \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.datanum\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # get data\n",
    "#         out_label = self.labelset[idx]\n",
    "#         out_data = self.dataset[idx]\n",
    "\n",
    "#         # to tensor\n",
    "#         out_label = torch.from_numpy(out_label).float()\n",
    "#         # out_data = torch.from_numpy(out_data).float()\n",
    "        \n",
    "#         # transform label\n",
    "#         # out_data = self.transform(out_data)\n",
    "#         # out_label = label_transform(out_label)\n",
    "\n",
    "#         # out_data = out_data.transpose(0, 1) # (dim, seq_len) => (seq_len, dim)\n",
    "#         # out_data = out_data.unsqueeze(0) # add channel for first convolution (seq_len, dim) => (c, seq_len, dim)\n",
    "#         # out_data = torch.stack([out_data, out_data, out_data]) # add channel for first convolution (seq_len, dim) => (c, seq_len, dim)\n",
    "#         # print(type(out_data))\n",
    "#         # print(type(np.array(out_label)))\n",
    "#         # print(out_data.shape)\n",
    "\n",
    "#         # # encoding on cpu(Its important for reduce usage of gpu memory)\n",
    "#         # out_data = out_data.unsqueeze(0) # add fake batch\n",
    "#         # out_data = model_efn.extract_features(out_data)\n",
    "#         # out_data = out_data[0]\n",
    "\n",
    "#         return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "np.ndarray.argmax(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: idx, out: batch(valid), label(s)\n",
    "class RainforestTrainDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = labels\n",
    "        self.ids = ids     \n",
    "        self.datanum = len(labels)   \n",
    "        self.record_ids = record_ids\n",
    "        self.offsets = offsets\n",
    "        self.augs = [\n",
    "            addNoisy,\n",
    "            contrast_stretching,\n",
    "            randomGaussian, \n",
    "            randomGamma,\n",
    "            nonAug\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get data\n",
    "        out_label = self.labels[idx]\n",
    "\n",
    "        # random crop\n",
    "        randomCropOffset = int((int(np.random.rand() * self.offsets[idx])))\n",
    "\n",
    "        # load wav\n",
    "        wavnp = np.load(Path('../input//rfcx-species-audio-detection/train_mel/' + str(self.ids[idx]) + '.npy'))\n",
    "\n",
    "        # transform\n",
    "        # wavnp = train_transform(wavnp)\n",
    "        \n",
    "        if randomCropOffset >= 0:\n",
    "            wavnp = wavnp[0 + randomCropOffset: (10 * params.sr) + randomCropOffset]\n",
    "        else:\n",
    "            wavnp = wavnp[len(wavnp) - (10 * params.sr) + randomCropOffset : len(wavnp) + randomCropOffset]\n",
    "        wavnp = wav2mel(wavnp) # 10s clipping\n",
    "\n",
    "        # aug= random.choice(self.augs)\n",
    "        # wavnp = aug(wavnp)\n",
    "\n",
    "        # dim, seq_len => seq_len, dim\n",
    "        wavnp = wavnp.T\n",
    "\n",
    "        # add channel\n",
    "        wavnp = np.stack([wavnp, wavnp, wavnp])\n",
    "\n",
    "        # to Tensor\n",
    "        wavTensor = torch.from_numpy(wavnp).float()\n",
    "\n",
    "        return wavTensor, out_label\n",
    "        # return wavTensor, np.ndarray.argmax(out_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: idx, out: batch(valid), label(s)\n",
    "class RainforestValidDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = labels\n",
    "        self.ids = ids     \n",
    "        self.datanum = len(labels)   \n",
    "        self.record_ids = record_ids\n",
    "        self.offsets = offsets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get data\n",
    "        out_label = self.labels[idx]\n",
    "\n",
    "        # random crop\n",
    "        randomCropOffset = int((int(np.random.rand() * self.offsets[idx])))\n",
    "\n",
    "        # load wav\n",
    "        wavnp = np.load(Path('../input//rfcx-species-audio-detection/train_mel/' + str(self.ids[idx]) + '.npy'))\n",
    "        if randomCropOffset >= 0:\n",
    "            wavnp = wavnp[0 + randomCropOffset: (10 * params.sr) + randomCropOffset]\n",
    "        else:\n",
    "            wavnp = wavnp[len(wavnp) - (10 * params.sr) + randomCropOffset : len(wavnp) + randomCropOffset]\n",
    "        wavnp = wav2mel(wavnp) # 10s clipping\n",
    "\n",
    "        # dim, seq_len => seq_len, dim\n",
    "        wavnp = wavnp.T\n",
    "\n",
    "        # add channel\n",
    "        wavnp = np.stack([wavnp, wavnp, wavnp])\n",
    "\n",
    "        # to Tensor\n",
    "        wavTensor = torch.from_numpy(wavnp).float()\n",
    "\n",
    "        return wavTensor, out_label\n",
    "        # return wavTensor, np.ndarray.argmax(out_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in: idx, out: batch(valid), label(s)\n",
    "# class RainforestValidDatasets(torch.utils.data.Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.labels = labels\n",
    "#         self.ids = ids     \n",
    "#         self.datanum = len(labels)   \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.datanum\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # get data\n",
    "#         out_label = self.labels[idx]\n",
    "\n",
    "#         outdatas = []\n",
    "\n",
    "#         #TODO: 11 is magic number, due to change it to conf\n",
    "#         for i in range(11):\n",
    "#             if i % 3 == 0:\n",
    "#                 # load melspec(dim, seq_len)\n",
    "#                 melspec =  np.load(os.path.join(config.VALID_AUDIO_ROOT, str(self.ids[idx]) + \"_\" + str(i) + \".npy\"))\n",
    "#                 # add channel\n",
    "#                 melspec = np.stack([melspec.T, melspec.T, melspec.T])\n",
    "#                 outdatas.append(melspec)\n",
    "\n",
    "#         # list 2 tochtensor(batch, channel, seq_len, dim)\n",
    "#         outdatas = torch.from_numpy(np.array(outdatas))\n",
    "\n",
    "#         return outdatas, out_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ds train\n",
    "# import librosa.display\n",
    "# ds = RainforestTrainDatasets(train_transform)\n",
    "# loader = DataLoader(ds)\n",
    "\n",
    "# # check aug\n",
    "# for x, y in loader:\n",
    "#     a = 1\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# ax.set(title='train random crop')\n",
    "# img = librosa.display.specshow(\n",
    "#     x.numpy()[0][0].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "\n",
    "\n",
    "# ds = RainforestDatasets(valid_transform)\n",
    "# loader = DataLoader(ds)\n",
    "\n",
    "# # check aug\n",
    "# for x, y in loader:\n",
    "#     a = 1\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# ax.set(title='validation center crop')\n",
    "# img = librosa.display.specshow(\n",
    "#     x.numpy()[0][0].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "\n",
    "# # ax.set(title=f'Mel-frequency spectrogram of {row[\"recording_id\"]}')\n",
    "# # fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "# print(x.numpy()[0][0].T.shape)\n",
    "\n",
    "# melspec = np.load(os.path.join(config.TRAIN_AUDIO_ROOT, str(1215) + \".npy\"))\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# img = librosa.display.specshow(\n",
    "#     melspec, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "# print(melspec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conformer\n",
    "# # https://arxiv.org/abs/2005.08100\n",
    "# class RainforestTransformer(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RainforestTransformer, self).__init__()         \n",
    "\n",
    "#         self.encoding = model_efn\n",
    "#         # self.pointwise = nn.Conv2d(config.ENC_CH, 1, (1, 1))\n",
    "#         self.conv = nn.Conv2d(config.ENC_CH, 1, (config.KERNEL_SIZE_SEQ, config.KERNEL_SIZE), stride=config.KERNEL_STRIDE)\n",
    "#         self.linear = nn.Linear(int((((((config.ENC_DIM - config.KERNEL_SIZE) / config.KERNEL_STRIDE) + 1) - config.POOL_SIZE) / config.POOL_STRIDE) + 1), config.ENC_DIM)\n",
    "#         self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "#         self.conformerblock = ConformerBlock(\n",
    "#             dim = config.ENC_DIM,\n",
    "#             dim_head = 64,\n",
    "#             heads = 8,\n",
    "#             ff_mult = 4,\n",
    "#             conv_expansion_factor = 2,\n",
    "#             conv_kernel_size = 31,\n",
    "#             attn_dropout = config.DROPOUT,\n",
    "#             ff_dropout = config.DROPOUT,\n",
    "#             conv_dropout = config.DROPOUT\n",
    "#         )\n",
    "#         self.conformerblock2 = ConformerBlock(\n",
    "#             dim = config.ENC_DIM,\n",
    "#             dim_head = 64,\n",
    "#             heads = 8,\n",
    "#             ff_mult = 4,\n",
    "#             conv_expansion_factor = 2,\n",
    "#             conv_kernel_size = 31,\n",
    "#             attn_dropout = config.DROPOUT,\n",
    "#             ff_dropout = config.DROPOUT,\n",
    "#             conv_dropout = config.DROPOUT\n",
    "#         )\n",
    "\n",
    "#         self.decoder = nn.Linear(1 * int((((((config.ENC_LEN - config.KERNEL_SIZE_SEQ) / config.KERNEL_STRIDE) + 1) -  config.POOL_SIZE) / config.POOL_STRIDE) + 1) * config.ENC_DIM, config.NUM_BIRDS)\n",
    "\n",
    "#         # devided by stride\n",
    "    \n",
    "#     # x: (b, c, seqlen, dim)\n",
    "#     def forward(self, x):\n",
    "#         # (b, c, seqlen, dim) => (b, c, seqlen, dim)\n",
    "#         x = self.encoding.extract_features(x)\n",
    "#         # enc = self.pointwise(enc)\n",
    "\n",
    "#         # (b, c, seqlen, dim) <= encoded matrix\n",
    "#         # point-wise convokution for convolution channel.\n",
    "#         h = F.relu(self.conv(x))\n",
    "#         h = F.max_pool2d(h, config.POOL_SIZE, stride=config.POOL_STRIDE)\n",
    "#         h = self.linear(h)\n",
    "#         h = h.transpose(0, 1)[0] # transpose batch and channel to delet channel dimension\n",
    "#         h = self.conformerblock(h)\n",
    "#         h = self.conformerblock2(h)\n",
    "#         # h = self.conformerblock3(h)\n",
    "#         # h = self.conformerblock4(h)\n",
    "#         # h = self.conformerblock5(h)\n",
    "#         # h = self.conformerblock6(h)\n",
    "#         h = h.view(-1, 1 * int((((((config.ENC_LEN - config.KERNEL_SIZE_SEQ) / config.KERNEL_STRIDE) + 1) -  config.POOL_SIZE) / config.POOL_STRIDE) + 1) * config.ENC_DIM)\n",
    "#         out = self.decoder(h)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 3, 5, 2])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "temp_img = torch.from_numpy(np.random.randn(5, 2))\n",
    "temp_ch = torch.stack([temp_img, temp_img, temp_img])\n",
    "temp_ch.shape\n",
    "temp_batch = torch.stack([temp_ch, temp_ch, temp_ch, temp_ch, temp_ch, temp_ch])\n",
    "print(temp_batch.shape)\n",
    "\n",
    "torch.mean(temp_batch, dim=3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy = torch.stack([clip, clip, clip]).unsqueeze(0)\n",
    "# model = RainforestTransformer()\n",
    "# y = model(sample.unsqueeze(0))\n",
    "# make_dot(y,params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserve Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete unusual var\n",
    "del sample\n",
    "# del model_efn\n",
    "# del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indiscating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5, 3, 10, 2)\n(4, 5, 3, 10, 2)\nloop all divnum\n(4, 3, 10, 2)\n0.0\n(4, 3, 10, 2)\n240.0\n(4, 3, 10, 2)\n0.0\n(4, 3, 10, 2)\n240.0\n(4, 3, 10, 2)\n0.0\n"
     ]
    }
   ],
   "source": [
    "allclip = np.stack([\n",
    "    np.zeros([3,10,2]),\n",
    "    np.ones([3,10,2]),\n",
    "    np.zeros([3,10,2]),\n",
    "    np.ones([3,10,2]),\n",
    "    np.zeros([3,10,2]),\n",
    "])\n",
    "# div, c, seq_len, dim\n",
    "print(allclip.shape)\n",
    "\n",
    "# b, d, c, s, d\n",
    "batch = np.stack([allclip, allclip, allclip, allclip])\n",
    "print(batch.shape)\n",
    "\n",
    "# reshape\n",
    "print('loop all divnum')\n",
    "for i in range(batch.shape[1]):\n",
    "    print(batch[:, i, :, :].shape)\n",
    "    # print(batch[])\n",
    "    print(batch[:, i, :, :].sum())\n",
    "    # 4 * 3 * 10 * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[4 0 9]\n",
      "  [5 8 0]\n",
      "  [9 2 6]\n",
      "  [3 8 2]]\n",
      "\n",
      " [[4 2 6]\n",
      "  [4 8 6]\n",
      "  [1 3 8]\n",
      "  [1 9 8]]]\n",
      "[[4 2 9]\n",
      " [5 8 6]\n",
      " [9 3 8]\n",
      " [3 9 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(0, 10, (2,4,3))\n",
    "print(a)\n",
    "\n",
    "print(np.max(a, axis=0))\n",
    "# print(np.max(a, axis=1))\n",
    "\n",
    "# print(np.max(a, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "torch.max(torch.Tensor([0,1,2]), dim=0).values"
   ]
  },
  {
   "source": [
    "## Custom Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adas(Optimizer):\n",
    "    r\"\"\"\n",
    "    Introduction:\n",
    "        For the mathematical part see https://github.com/YanaiEliyahu/AdasOptimizer,\n",
    "        the `Theory` section contains the major innovation,\n",
    "        and then `How ADAS works` contains more low level details that are still somewhat related to the theory.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n",
    "        lr: float > 0. Initial learning rate that is per feature/input (e.g. dense layer with N inputs and M outputs, will have N learning rates).\n",
    "        lr2: float >= 0.  lr's Initial learning rate. (just ~1-2 per layer, additonal one because of bias)\n",
    "        lr3: float >= 0. lr2's fixed learning rate. (global)\n",
    "        beta_1: 0 < float < 1. Preferably close to 1. Second moments decay factor to update lr and lr2 weights.\n",
    "        beta_2: 0 < float < 1. Preferably close to 1. 1/(1 - beta_2) steps back in time that `lr`s will be optimized for, larger dataset might require more nines.\n",
    "        beta_3: 0 < float < 1. Preferably close to 1. Same as beta_2, but for `lr2`s.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params,\n",
    "            lr = 0.001, lr2 = .005, lr3 = .0005,\n",
    "            beta_1 = 0.999, beta_2 = 0.999, beta_3 = 0.9999,\n",
    "            epsilon = 1e-8, **kwargs):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid lr: {}\".format(lr))\n",
    "        if not 0.0 <= lr2:\n",
    "            raise ValueError(\"Invalid lr2: {}\".format(lr))\n",
    "        if not 0.0 <= lr3:\n",
    "            raise ValueError(\"Invalid lr3: {}\".format(lr))\n",
    "        if not 0.0 <= epsilon:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= beta_1 < 1.0:\n",
    "            raise ValueError(\"Invalid beta_1 parameter: {}\".format(betas[0]))\n",
    "        if not 0.0 <= beta_2 < 1.0:\n",
    "            raise ValueError(\"Invalid beta_2 parameter: {}\".format(betas[1]))\n",
    "        if not 0.0 <= beta_3 < 1.0:\n",
    "            raise ValueError(\"Invalid beta_3 parameter: {}\".format(betas[2]))\n",
    "        defaults = dict(lr=lr, lr2=lr2, lr3=lr3, beta_1=beta_1, beta_2=beta_2, beta_3=beta_3, epsilon=epsilon)\n",
    "        self._varn = None\n",
    "        self._is_create_slots = None\n",
    "        self._curr_var = None\n",
    "        self._lr = lr\n",
    "        self._lr2 = lr2\n",
    "        self._lr3 = lr3\n",
    "        self._beta_1 = beta_1\n",
    "        self._beta_2 = beta_2\n",
    "        self._beta_3 = beta_3\n",
    "        self._epsilon = epsilon\n",
    "        super(Adas, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adas, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _add(self,x,y):\n",
    "        x.add_(y)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    # TODO: fix variables' names being too convoluted in _derivatives_normalizer and _get_updates_universal_impl\n",
    "    def _derivatives_normalizer(self,derivative,beta):\n",
    "        steps = self._make_variable(0,(),derivative.dtype)\n",
    "        self._add(steps,1)\n",
    "        factor = (1. - (self._beta_1 ** steps)).sqrt()\n",
    "        m = self._make_variable(0,derivative.shape,derivative.dtype)\n",
    "        moments = self._make_variable(0,derivative.shape,derivative.dtype)\n",
    "        m.mul_(self._beta_1).add_((1 - self._beta_1) * derivative * derivative)\n",
    "        np_t = derivative * factor / (m.sqrt() + self._epsilon)\n",
    "        #the third returned value should be called when the moments is finally unused, so it's updated\n",
    "        return (moments,np_t,lambda: moments.mul_(beta).add_((1 - beta) * np_t))\n",
    "\n",
    "    def _make_variable(self,value,shape,dtype):\n",
    "        self._varn += 1\n",
    "        name = 'unnamed_variable' + str(self._varn)\n",
    "        if self._is_create_slots:\n",
    "            self.state[self._curr_var][name] = torch.full(size=shape,fill_value=value,dtype=dtype,device=self._curr_var.device)\n",
    "        return self.state[self._curr_var][name]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_updates_universal_impl(self, grad, param):\n",
    "        lr = self._make_variable(value = self._lr,shape=param.shape[1:], dtype=param.dtype)\n",
    "        moment, deriv, f = self._derivatives_normalizer(grad,self._beta_3)\n",
    "        param.add_( - torch.unsqueeze(lr,0) * deriv)\n",
    "        lr_deriv = torch.sum(moment * grad,0)\n",
    "        f()\n",
    "        master_lr = self._make_variable(self._lr2,(),dtype=torch.float32)\n",
    "        m2,d2, f = self._derivatives_normalizer(lr_deriv,self._beta_2)\n",
    "        self._add(lr,master_lr * lr * d2)\n",
    "        master_lr_deriv2 = torch.sum(m2 * lr_deriv)\n",
    "        f()\n",
    "        m3,d3,f = self._derivatives_normalizer(master_lr_deriv2,0.)\n",
    "        self._add(master_lr,self._lr3 * master_lr * d3)\n",
    "        f()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_updates_universal(self, param, grad, is_create_slots):\n",
    "        self._curr_var = param\n",
    "        self._is_create_slots = is_create_slots\n",
    "        self._varn = 0\n",
    "        return self._get_updates_universal_impl(grad,self._curr_var.data)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adas does not support sparse gradients')\n",
    "                self._get_updates_universal(p,grad,len(self.state[p]) == 0)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED)\n",
    "    # msss = MultilabelStratifiedShuffleSplit(n_splits=config.N_FOLDS, test_size=config.TEST_SIZE, random_state=config.SEED)\n",
    "\n",
    "    # Read dataset\n",
    "    train_datasets = RainforestTrainDatasets()\n",
    "    # valid_datasets = RainforestDatasets(_transform=valid_transform)\n",
    "\n",
    "    # valid dataset dosen't need to transform(already be croped)\n",
    "    valid_datasets = RainforestValidDatasets()\n",
    "\n",
    "    best_epochs = []\n",
    "    best_lwlraps = []\n",
    "\n",
    "    # tensorboard\n",
    "    # if IS_WRITRE_LOG:\n",
    "    #     writer = SummaryWriter(log_dir=\"./logs/\" + EXP_NAME)\n",
    "\n",
    "    # k-fold\n",
    "    # for kfoldidx, (train_index, valid_index) in enumerate(msss.split(labels, labels)):\n",
    "    for kfoldidx, (train_index, valid_index) in enumerate(skf.split(specIds, specIds)):\n",
    "\n",
    "        # # model \n",
    "        # model = RainforestTransformer()\n",
    "        # model.to(device)\n",
    "        # model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        model = get_model()\n",
    "        # num_ftrs = model._fc.in_features\n",
    "        # model._fc = nn.Linear(num_ftrs, config.NUM_BIRDS)\n",
    "        model.to(device)\n",
    "\n",
    "        np.save('train_index_fold_' + str(kfoldidx), np.array(train_index))\n",
    "        np.save('valid_index_fold_' + str(kfoldidx), np.array(valid_index))\n",
    "\n",
    "        # init\n",
    "        best_lwlrap = 0.\n",
    "        best_epoch = 0\n",
    "\n",
    "        if IS_WRITRE_LOG:\n",
    "            run = wandb.init(config=config, project=PROJECT, group=EXP_NAME, reinit=True)\n",
    "            print('wandb init')\n",
    "            wandb.run.name = EXP_NAME + '-fold-' + str(kfoldidx)\n",
    "            wandb.run.save()\n",
    "            wandb.watch(model)\n",
    "\n",
    "        # criterion\n",
    "        print('wandb init2')\n",
    "        criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "        # criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        # optimizer\n",
    "        # optimizer = Adam(params=model.parameters(), lr=config.lr, amsgrad=False)\n",
    "        optimizer = toptim.RAdam(\n",
    "            model.parameters(),\n",
    "            lr=config.lr,\n",
    "            betas=config.betas,\n",
    "            eps=config.eps,\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "        # optimizer = Adas(model.parameters(), lr=config.lr)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "        # print(optimizer)\n",
    "\n",
    "        # train\n",
    "        train_subset = Subset(train_datasets, train_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=config.BATCH_NUM, shuffle=True)\n",
    "\n",
    "        # validation\n",
    "        valid_subset = Subset(valid_datasets, valid_index)\n",
    "        valid_loader = DataLoader(valid_subset, batch_size=config.VALID_BATCH_NUM, shuffle=False)\n",
    "\n",
    "        # # scheduler\n",
    "        # # scheduler = CosineAnnealingLR(optimizer, T_max=config.t_max, eta_min=config.eta_min)\n",
    "        # num_train_steps = int(len(train_loader) * config.EPOCH_NUM)\n",
    "        # num_warmup_steps = int(0.1 * config.EPOCH_NUM * len(train_loader))\n",
    "        # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "\n",
    "        # epoch\n",
    "        mb = master_bar(range(config.EPOCH_NUM))\n",
    "        mb.names = ['avg_loss', 'avg_val_loss', 'lwlrap']\n",
    "\n",
    "        # Epoch\n",
    "        for epoch in mb:\n",
    "\n",
    "            # start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train mode\n",
    "            model.train()\n",
    "\n",
    "            # init loss\n",
    "            avg_loss = 0.\n",
    "\n",
    "            # batch training\n",
    "            train_batch_preds = []\n",
    "            train_batch_labels = []\n",
    "            for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "\n",
    "                # MixUp\n",
    "                dice = np.random.rand(1)\n",
    "                if dice < config.MIXUP_PROB:\n",
    "                    # mixup\n",
    "                    x_batch, y_batch, y_batch_b, lam = mixup_data(x_batch, y_batch, alpha=config.MIXUP, use_cuda=True)\n",
    "\n",
    "                # spec Aug\n",
    "                dice_s = np.random.rand(1)\n",
    "                if dice_s < config.SPEC_PROB:\n",
    "                    # specaug\n",
    "                    x_batch = spec_augmenter(x_batch)                \n",
    "\n",
    "                # forward\n",
    "                preds = model(x_batch.to(device))\n",
    "\n",
    "                if dice < config.MIXUP_PROB:\n",
    "                    loss = mixup_criterion(criterion, preds, y_batch.to(device), y_batch_b.to(device), lam)\n",
    "                else:\n",
    "                    loss = criterion(preds, y_batch.to(device)) # It dosen't need Sigmoid, because BCE includes sigmoid function.\n",
    "\n",
    "                # loss = criterion(preds, y_batch.to(device))\n",
    "                # loss = criterion(preds, torch.max(y_batch, dim=1).indices.to(device, dtype=torch.long)) # It dosen't need Sigmoid, because BCE includes sigmoid function.\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "                del loss\n",
    "\n",
    "                # add preds\n",
    "                train_batch_preds.extend(torch.sigmoid(preds).detach().cpu().numpy().tolist())\n",
    "                train_batch_labels.extend(y_batch.detach().cpu().numpy().tolist())\n",
    "\n",
    "            # calc score\n",
    "            score, weight = calculate_per_class_lwlrap(np.array(train_batch_labels), np.array(train_batch_preds))\n",
    "            train_lwlrap = (score * weight).sum()\n",
    "\n",
    "            # last_preds =  np.array(train_batch_preds)\n",
    "            # last_labels = np.array(train_batch_labels)\n",
    "\n",
    "            # validation mode\n",
    "            model.eval()\n",
    "            valid_batch_preds = []\n",
    "            valid_batch_labels = []\n",
    "            # valid_preds = np.zeros((len(valid_index), config.NUM_BIRDS))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation\n",
    "            # for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "            for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                # !!!caution!!!\n",
    "                # x_batch's shape (batch, devide length(i.e. 51), channel, seq_len, dim)\n",
    "                # extract column\n",
    "\n",
    "                # wholeclip_preds = []\n",
    "            \n",
    "                # for divnum in range(x_batch.shape[1]):\n",
    "                #     x_batch_divided = x_batch[:, divnum, :, :]\n",
    "                #     preds = model(x_batch_divided.to(device)).detach() # (batch, species_id)\n",
    "                #     wholeclip_preds.append(preds.cpu().numpy().tolist()) # (divnum, batch, species_id)\n",
    "\n",
    "                # # get max via divnum\n",
    "                # # (batch, preds_dimention)\n",
    "                # preds = torch.max(torch.from_numpy(np.array(wholeclip_preds)).float(), dim=0).values\n",
    "                preds = model(x_batch.to(device)).detach() # (batch, species_id)\n",
    "\n",
    "                # preds = model(x_batch.to(device)).detach()\n",
    "                loss = criterion(preds.to(device), y_batch.to(device))\n",
    "                # loss = criterion(preds.to(device), torch.max(y_batch, dim=1).indices.to(device, dtype=torch.long))\n",
    "\n",
    "                preds = torch.sigmoid(preds)\n",
    "                # valid_preds[i * config.VALID_BATCH_NUM: (i+1) * config.VALID_BATCH_NUM] = preds.cpu().numpy()\n",
    "                avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "                valid_batch_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "                valid_batch_labels.extend(y_batch.detach().cpu().tolist())\n",
    "\n",
    "            # calc score\n",
    "            # score, weight = calculate_per_class_lwlrap(labels[valid_index], valid_preds)\n",
    "            score, weight = calculate_per_class_lwlrap(np.array(valid_batch_labels), np.array(valid_batch_preds))\n",
    "            lwlrap = (score * weight).sum()\n",
    "\n",
    "            # update lr\n",
    "            # scheduler.step()\n",
    "            # scheduler.step(avg_val_loss)\n",
    "\n",
    "            # tensorboard\n",
    "            if IS_WRITRE_LOG:\n",
    "                # tensorboard\n",
    "                # writer.add_scalar(\"train_loss/fold-\" + str(kfoldidx), avg_loss, epoch)\n",
    "                # writer.add_scalar(\"valid_loss/fold-\" + str(kfoldidx), avg_val_loss, epoch)\n",
    "                # writer.add_scalar(\"train_lwlrap/fold-\" + str(kfoldidx), train_lwlrap, epoch)\n",
    "                # writer.add_scalar(\"valid_lwlrap/fold-\" + str(kfoldidx), lwlrap, epoch)\n",
    "\n",
    "                wandb.log({\n",
    "                    'loss/train': avg_loss,\n",
    "                    'lwlrap/train': train_lwlrap,\n",
    "                    'loss/validation': avg_val_loss,\n",
    "                    'lwlrap/validation': lwlrap,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} train_lwlrap: {train_lwlrap:.6f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n",
    "        \n",
    "            if lwlrap > best_lwlrap and epoch > 10:\n",
    "                best_epoch = epoch + 1\n",
    "                best_lwlrap = lwlrap\n",
    "                # torch.save(model.state_dict(), 'weight_best_' + str(EXP_NUM) + '_fold' + str(kfoldidx) +'.pt')\n",
    "                torch.save(model.state_dict(), 'weight_best_fold' + str(kfoldidx) +'.pt')\n",
    "                np.save('train_batch_preds_' + str(kfoldidx), np.array(train_batch_preds))\n",
    "                np.save('train_batch_labels_' + str(kfoldidx), np.array(train_batch_labels))\n",
    "                np.save('valid_batch_preds_' + str(kfoldidx), np.array(valid_batch_preds))\n",
    "                np.save('valid_batch_labels_' + str(kfoldidx), np.array(valid_batch_labels))\n",
    "            \n",
    "        best_epochs.append(best_epoch)\n",
    "        best_lwlraps.append(best_lwlrap)\n",
    "\n",
    "        # return last_preds, last_labels\n",
    "\n",
    "        if IS_WRITRE_LOG:\n",
    "            run.finish()\n",
    "\n",
    "    # if IS_WRITRE_LOG:\n",
    "    #     writer.close()\n",
    "    \n",
    "    return {\n",
    "        'best_epoch': best_epochs,\n",
    "        'best_lwlrap': best_lwlraps,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: Currently logged in as: tatsuya-takahashi (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.18 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.17<br/>\n                Syncing run <strong style=\"color:#cdcd00\">drawn-morning-237</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX</a><br/>\n                Run page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/p7ayphcd\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/p7ayphcd</a><br/>\n                Run data is saved locally in <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_163951-p7ayphcd</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "wandb init\n",
      "wandb init2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 25280<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.03MB of 0.03MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec4204681b3c49fb85145ba61be38602"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_163951-p7ayphcd\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_163951-p7ayphcd\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td>0.0202</td></tr><tr><td>lwlrap/train</td><td>0.96537</td></tr><tr><td>loss/validation</td><td>0.07264</td></tr><tr><td>lwlrap/validation</td><td>0.81501</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>_step</td><td>29</td></tr><tr><td>_runtime</td><td>2914</td></tr><tr><td>_timestamp</td><td>1613204905</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td></td></tr><tr><td>lwlrap/train</td><td></td></tr><tr><td>loss/validation</td><td></td></tr><tr><td>lwlrap/validation</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">drawn-morning-237</strong>: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/p7ayphcd\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/p7ayphcd</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: wandb version 0.10.18 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.17<br/>\n                Syncing run <strong style=\"color:#cdcd00\">feasible-bee-238</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX</a><br/>\n                Run page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/1kj190a6\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/1kj190a6</a><br/>\n                Run data is saved locally in <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_172832-1kj190a6</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "wandb init\n",
      "wandb init2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 16004<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.08MB of 0.08MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61c41c9172c14df8b132aa3eaf119a03"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_172832-1kj190a6\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_172832-1kj190a6\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td>0.01952</td></tr><tr><td>lwlrap/train</td><td>0.96838</td></tr><tr><td>loss/validation</td><td>0.08448</td></tr><tr><td>lwlrap/validation</td><td>0.83702</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>_step</td><td>29</td></tr><tr><td>_runtime</td><td>2954</td></tr><tr><td>_timestamp</td><td>1613207866</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td></td></tr><tr><td>lwlrap/train</td><td></td></tr><tr><td>loss/validation</td><td></td></tr><tr><td>lwlrap/validation</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">feasible-bee-238</strong>: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/1kj190a6\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/1kj190a6</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: wandb version 0.10.18 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.17<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lyric-blaze-239</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX</a><br/>\n                Run page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/3qjvyg5y\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/3qjvyg5y</a><br/>\n                Run data is saved locally in <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_181754-3qjvyg5y</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "wandb init\n",
      "wandb init2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 25924<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.08MB of 0.08MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5012b23ee77d476993a8198f8088f298"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_181754-3qjvyg5y\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_181754-3qjvyg5y\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td>0.01169</td></tr><tr><td>lwlrap/train</td><td>0.9854</td></tr><tr><td>loss/validation</td><td>0.10291</td></tr><tr><td>lwlrap/validation</td><td>0.8182</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>_step</td><td>29</td></tr><tr><td>_runtime</td><td>2948</td></tr><tr><td>_timestamp</td><td>1613210822</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td></td></tr><tr><td>lwlrap/train</td><td></td></tr><tr><td>loss/validation</td><td></td></tr><tr><td>lwlrap/validation</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lyric-blaze-239</strong>: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/3qjvyg5y\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/3qjvyg5y</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: wandb version 0.10.18 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.17<br/>\n                Syncing run <strong style=\"color:#cdcd00\">easy-galaxy-240</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX</a><br/>\n                Run page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/8qsmd3l8\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/8qsmd3l8</a><br/>\n                Run data is saved locally in <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_190710-8qsmd3l8</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "wandb init\n",
      "wandb init2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 12692<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.08MB of 0.08MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56c54234e38b4861a2e9046200e59d22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_190710-8qsmd3l8\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_190710-8qsmd3l8\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td>0.01657</td></tr><tr><td>lwlrap/train</td><td>0.97237</td></tr><tr><td>loss/validation</td><td>0.12236</td></tr><tr><td>lwlrap/validation</td><td>0.8026</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>_step</td><td>29</td></tr><tr><td>_runtime</td><td>2938</td></tr><tr><td>_timestamp</td><td>1613213768</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td></td></tr><tr><td>lwlrap/train</td><td></td></tr><tr><td>loss/validation</td><td></td></tr><tr><td>lwlrap/validation</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">easy-galaxy-240</strong>: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/8qsmd3l8\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/8qsmd3l8</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: wandb version 0.10.18 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.17<br/>\n                Syncing run <strong style=\"color:#cdcd00\">astral-donkey-241</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX</a><br/>\n                Run page: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/2l6vvrtr\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/2l6vvrtr</a><br/>\n                Run data is saved locally in <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_195616-2l6vvrtr</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: WARNING Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "wandb init\n",
      "wandb init2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 25640<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.03MB of 0.03MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c56555adae4067bbf29f654f6de3fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_195616-2l6vvrtr\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>d:\\000_devs\\003_kaggle_Rainforest-Connection-Species-Audio-Detection\\Rainforest-Connection-Species-Audio-Detection\\src\\wandb\\run-20210213_195616-2l6vvrtr\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td>0.01395</td></tr><tr><td>lwlrap/train</td><td>0.9783</td></tr><tr><td>loss/validation</td><td>0.12142</td></tr><tr><td>lwlrap/validation</td><td>0.75186</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>_step</td><td>29</td></tr><tr><td>_runtime</td><td>3047</td></tr><tr><td>_timestamp</td><td>1613216823</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>loss/train</td><td></td></tr><tr><td>lwlrap/train</td><td></td></tr><tr><td>loss/validation</td><td></td></tr><tr><td>lwlrap/validation</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>_step</td><td></td></tr><tr><td>_runtime</td><td></td></tr><tr><td>_timestamp</td><td></td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">astral-donkey-241</strong>: <a href=\"https://wandb.ai/tatsuya-takahashi/RFCX/runs/2l6vvrtr\" target=\"_blank\">https://wandb.ai/tatsuya-takahashi/RFCX/runs/2l6vvrtr</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'best_epoch': [20, 19, 21, 15, 26], 'best_lwlrap': [0.8647248682369789, 0.8458768677351565, 0.8566960317460317, 0.8353679394535425, 0.8739873252277832]}\n"
     ]
    }
   ],
   "source": [
    "result = train()\n",
    "print(result)"
   ]
  },
  {
   "source": [
    "### Folds Analytics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(973, 24)\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n(973, 24)\n[7.92401209e-02 7.07243616e-03 8.82122025e-04 5.98729309e-03\n 1.49917451e-03 1.78665039e-03 4.01552441e-03 1.00857508e-03\n 3.32118711e-03 1.35328423e-03 2.67865881e-03 1.10318745e-03\n 3.74864973e-03 5.41937712e-04 2.33639055e-03 1.59019895e-03\n 2.40515583e-04 2.00599106e-03 4.13687062e-03 6.13338896e-04\n 1.69306062e-03 1.69254350e-03 3.89699116e-02 8.28363597e-01]\n(243, 24)\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n(243, 24)\n[0.02721454 0.00502928 0.00421348 0.7744838  0.234465   0.0050208\n 0.02447076 0.00308966 0.0142504  0.00221401 0.00078811 0.00429609\n 0.7165339  0.124419   0.01968522 0.00883205 0.00176169 0.01069615\n 0.14294885 0.00736105 0.00302747 0.0098618  0.0013848  0.01378525]\n"
     ]
    }
   ],
   "source": [
    "# calc lwlrap\n",
    "train_batch_labels = np.load(\"./train_batch_labels.npy\")\n",
    "train_batch_preds = np.load(\"./train_batch_preds.csv.npy\")\n",
    "# extrct under < 1.0\n",
    "valid_batch_labels = np.load(\"./valid_batch_labels.npy\")\n",
    "valid_batch_preds = np.load(\"./valid_batch_preds.csv.npy\")\n",
    "\n",
    "\n",
    "# \n",
    "print(train_batch_labels.shape)\n",
    "print(train_batch_labels[0])\n",
    "\n",
    "print(train_batch_preds.shape)\n",
    "print(train_batch_preds[0])\n",
    "\n",
    "print(valid_batch_labels.shape)\n",
    "print(valid_batch_labels[0])\n",
    "\n",
    "print(valid_batch_preds.shape)\n",
    "print(valid_batch_preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(973, 24)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "models = []\n",
    "for fold in range(config.N_FOLDS):\n",
    "   #  if not fold == 0:\n",
    "    # load network\n",
    "    print(fold)\n",
    "   #  model = RainforestTransformer()\n",
    "   #  model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    model = get_model()\n",
    "    # num_ftrs = model._fc.in_features\n",
    "    # model._fc = nn.Linear(num_ftrs, config.NUM_BIRDS)\n",
    "\n",
    "    # torch.save(model.state_dict(), 'weight_best_' + str(EXP_NUM) + '_fold' + str(kfoldidx) +'.pt')\n",
    "    model.load_state_dict(torch.load('weight_best_fold' + str(fold) +'.pt'))\n",
    "    # print('weight_best_' + str(EXP_NUM) + '_fold' + str(fold) +'.pt')\n",
    "    # model.load_state_dict(torch.load('weight_best_' + str(EXP_NUM) + '_fold' + str(fold) +'.pt'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "submission_exp_28_fullMultiLabel.csv\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1992.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ecdc8e6f4c148ceb7098dce5ef6a871"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted for 100 of 1993 files\n",
      "Predicted for 200 of 1993 files\n",
      "Predicted for 300 of 1993 files\n",
      "Predicted for 400 of 1993 files\n",
      "Predicted for 500 of 1993 files\n",
      "Predicted for 600 of 1993 files\n",
      "Predicted for 700 of 1993 files\n",
      "Predicted for 800 of 1993 files\n",
      "Predicted for 900 of 1993 files\n",
      "Predicted for 1000 of 1993 files\n",
      "Predicted for 1100 of 1993 files\n",
      "Predicted for 1200 of 1993 files\n",
      "Predicted for 1300 of 1993 files\n",
      "Predicted for 1400 of 1993 files\n",
      "Predicted for 1500 of 1993 files\n",
      "Predicted for 1600 of 1993 files\n",
      "Predicted for 1700 of 1993 files\n",
      "Predicted for 1800 of 1993 files\n",
      "Predicted for 1900 of 1993 files\n",
      "\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# write submission\n",
    "with open('submission_' + EXP_NAME + '.csv', 'w', newline='') as csvfile:\n",
    "# with open('submission_' + EXP_NAME + '_sum.csv', 'w', newline='') as csvfile:\n",
    "    print('submission_' + EXP_NAME + '.csv')\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    \n",
    "    test_files = os.listdir(config.TEST_AUDIO_FLAC)\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in tqdm(range(0, len(test_files))):\n",
    "    # for i in range(0, 1):\n",
    "        # read data\n",
    "        # X_test = torch.from_numpy(np.load(os.path.join(config.TEST_AUDIO_ROOT, test_files[i])))\n",
    "\n",
    "        # (dim, seq_len)        \n",
    "        # X_test = np.load(os.path.join(config.TEST_AUDIO_ROOT, test_files[i]))\n",
    "        X_test_batch = []\n",
    "\n",
    "        # muptiply number\n",
    "        # TODO: 51 is magic number. You have to rewrite 51 to vars.\n",
    "        dev_num = 6\n",
    "\n",
    "        # Cutting!\n",
    "        for idx in range(dev_num):\n",
    "            recId =  test_files[i].split('.')[0]\n",
    "            X_test = np.load(os.path.join(config.TEST_AUDIO_ROOT, recId + '_' + str(idx) + '.npy')) # (DIM, seq_len)\n",
    "            X_test_clip = X_test.T # (seq_len, DIM)\n",
    "            # X_test_clip = X_test_clip[np.newaxis, :, :] # add fake channel\n",
    "            X_test_clip = np.stack([X_test_clip, X_test_clip, X_test_clip]) # expand to channel\n",
    "            X_test_batch.append(X_test_clip.tolist())\n",
    "\n",
    "        # to_tensor\n",
    "        X_test_batch = torch.from_numpy(np.array(X_test_batch)).float() # (batch, channel, seq_len, dim)\n",
    "        X_test_batch = X_test_batch.to(device)\n",
    "\n",
    "        # predict\n",
    "        output_list = []\n",
    "        for m in models:\n",
    "            outputs = []\n",
    "            for x_b in X_test_batch:\n",
    "                output = m(torch.stack([x_b]))\n",
    "                outputs.append(output[0].cpu().detach().numpy().tolist())\n",
    "            # outputs S= m(X_test_batch)\n",
    "            maxed_output = torch.max(torch.from_numpy(np.array(outputs)).float(), dim=0) # max about batch clips\n",
    "            # maxed_output = torch.sum(torch.from_numpy(np.array(outputs)).float(), dim=0) # max about batch clips\n",
    "            # maxed_output = torch.max(outputs, dim=0) # max about batch clips\n",
    "            maxed_output = maxed_output.values.cpu().detach()\n",
    "            # maxed_output = maxed_output.cpu().detach()\n",
    "            output_list.append(maxed_output)\n",
    "        avg_maxed_output = torch.mean(torch.stack(output_list), dim=0)\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "#         for out in maxed_output:\n",
    "        for out in avg_maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "        \n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-31.3091, -21.3795, -35.3548,  -6.3254, -40.4497, -23.6526, -32.6754,\n",
       "        -32.7722, -36.0916, -36.9664, -31.6402, -32.9257, -13.0157, -37.1099,\n",
       "        -26.1666, -26.2483, -34.4011, -31.6482, -32.1397, -32.3846, -32.3673,\n",
       "        -33.4588, -22.6971, -23.2646])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "maxed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 3, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.8104,  0.9677, -0.0584,  0.3181, -0.9070],\n",
       "        [-1.9207, -0.5634,  0.7859,  1.7470, -0.5243],\n",
       "        [ 0.0195, -1.2085,  0.4255,  0.9362,  0.2224]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.3029, -1.2753, -0.4758,  2.3839,  0.9157],\n        [-0.6430,  0.7113,  0.4000, -1.2039, -0.4198],\n        [-1.1929, -0.9351,  0.2138, -1.2842, -0.6917]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([3, 1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "a = torch.randn(3, 5)\n",
    "print(a)\n",
    "torch.max(a, dim=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "27aea0cd8eeb85eb7f1f0c1d3ddb84030b7fdf2e16880050672f3a8bc687c466"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}