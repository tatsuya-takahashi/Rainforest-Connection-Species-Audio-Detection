{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Based on clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "expname:exp_15_RealtimeTransform\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT = \"RFCX\"\n",
    "EXP_NUM = \"15\"\n",
    "EXP_TITLE = \"RealtimeTransform\"\n",
    "EXP_NAME = \"exp_\" + EXP_NUM + \"_\" + EXP_TITLE\n",
    "IS_WRITRE_LOG = True\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'train_clip'\n",
    "print('expname:' + EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import os\n",
    "# import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import wandb\n",
    "from time import sleep\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.utils.data as torchdata\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional\n",
    "from numpy.random import beta\n",
    "from pathlib import Path\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from torchviz import make_dot\n",
    "from conformer import ConformerConvModule\n",
    "from conformer import ConformerBlock\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "import librosa as lb\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.scroll_box { height:90em  !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# load weight\n",
    "# model_efn = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "model_efn = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "# model_efn = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "model_efn.to(device); # calculate on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(931839,)\n(320, 938)\n938 320\n"
     ]
    }
   ],
   "source": [
    "# 5get length\n",
    "class params:\n",
    "    sr = 48000\n",
    "    n_mels = 320\n",
    "    fmin = 40\n",
    "    fmax = sr // 2\n",
    "    fft = 2048\n",
    "    hop = 512\n",
    "    clip_frame = 10 * 48000\n",
    "    augnum = 100\n",
    "\n",
    "def wav2mel(wavnp):\n",
    "    melspec = lb.feature.melspectrogram(\n",
    "        wavnp, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax, n_fft=params.fft, hop_length=params.hop, \n",
    "    )\n",
    "    melspec = lb.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "    # normalize\n",
    "    melspec = melspec - np.min(melspec)\n",
    "    melspec = melspec / np.max(melspec)\n",
    "    return melspec\n",
    "\n",
    "wavnp = np.load(Path('../input//rfcx-species-audio-detection/train_mel/0.npy'))\n",
    "print(wavnp.shape)\n",
    "sample = wav2mel(wavnp[0: 10 * params.sr]) # 10s clipping\n",
    "\n",
    "# sample data\n",
    "# sample = torch.load(Path(\"e:/002_datasets/000_RFCX/train_mel_clip_aug/0_0.pt\"))\n",
    "# sample = torch.from_numpy(np.load(Path(\"../input/rfcx-species-audio-detection/train_mel/0.npy\")))\n",
    "# sample = torch.load(Path(\"../input/rfcx-species-audio-detection/train_mel_clip_aug/0_0.pt\"))\n",
    "\n",
    "# channel, seq, dim\n",
    "print(sample.shape)\n",
    "# print(sample[np.newaxis, :, :].shape)\n",
    "clip_len = int(sample.shape[1])\n",
    "clip_dim = int(sample.shape[0])\n",
    "print(clip_len, clip_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clip (938, 320)\n",
      "img torch.Size([1, 3, 938, 320])\n",
      "enc torch.Size([1, 1280, 30, 10])\n",
      "ch, enc_len, enc_dim 1280 30 10\n"
     ]
    }
   ],
   "source": [
    "# expeliment\n",
    "clip = sample.T\n",
    "print(\"clip\", clip.shape)\n",
    "\n",
    "# stacking\n",
    "img = torch.from_numpy(np.array([\n",
    "        [clip],[clip],[clip]\n",
    "    ])).float().transpose(0, 1)\n",
    "print(\"img\", img.shape)\n",
    "\n",
    "# encoding\n",
    "enc = model_efn.extract_features(img.to(device))\n",
    "print(\"enc\", enc.shape)\n",
    "\n",
    "enc = enc.detach().cpu()\n",
    "\n",
    "# save\n",
    "ch = enc.shape[1]\n",
    "enc_len = enc.shape[2]\n",
    "enc_dim = enc.shape[3]\n",
    "print('ch, enc_len, enc_dim', ch, enc_len, enc_dim)\n",
    "\n",
    "del enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dict2(dict): \n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__(*args, **kwargs) \n",
    "        self.__dict__ = self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict2({\n",
    "    \"fft\":                2048,\n",
    "    \"hop\":                512,\n",
    "    \"sr\":                 48000,\n",
    "    \"mel\":                320,\n",
    "    \"SEED\":               42,\n",
    "    # \"INPUT\":              Path(\"../input/rfcx-species-audio-detection/train\"),\n",
    "    # \"TRAIN_AUDIO_ROOT\":   Path(\"../input/rfcx-species-audio-detection/train_mel_clip_aug/\"),\n",
    "    # \"TEST_AUDIO_ROOT\":    Path\"../input/rfcx-species-audio-detection/train_mel_clip_aug/0_0.pt\"),\n",
    "    # \"TRAIN_TP\":           Path(\"../input/rfcx-species-audio-detection/train_tp.csv\"),\n",
    "    # \"TRAIN_TP_MEL\":       Path(\"../input/rfcx-species-audio-detection/train_tp_mel.csv\"),\n",
    "    # \"SUB\":                Path(\"../input/rfcx-species-audio-detection/sample_submission.csv\"),\n",
    "    \"TRAIN_AUDIO_ROOT\":   Path(\"e:/002_datasets/000_RFCX/train_mel_clip_aug/\"),\n",
    "    \"TEST_AUDIO_ROOT\":    Path(\"e:/002_datasets/000_RFCX/test_mel_clip/\"),\n",
    "    \"VALID_AUDIO_ROOT\":   Path(\"e:/002_datasets/000_RFCX/valid_mel_clip/\"),\n",
    "    \"TRAIN_TP\":           Path(\"../input/rfcx-species-audio-detection/train_tp.csv\"),\n",
    "    \"TRAIN_TP_CSV\":       Path(\"../input/rfcx-species-audio-detection/train_tp_mel.csv\"),\n",
    "    \"VALID_CSV\":          Path(\"../input/rfcx-species-audio-detection/valid.csv\"),\n",
    "    \"TEST_CSV\":           Path(\"../input/rfcx-species-audio-detection/test.csv\"),\n",
    "    \"SUB\":                Path(\"../input/rfcx-species-audio-detection/sample_submission.csv\"),\n",
    "    # \"DIM\":                sample.shape[0],\n",
    "    # \"SEQ_LEN\":            int(sample.shape[1] * 0.8),\n",
    "    # \"DIM\":                dim,\n",
    "    # \"ENC_LEN\":            seq_len,\n",
    "    \"CLIP_LEN\":           clip_len,\n",
    "    \"CLIP_DIM\":           clip_dim,\n",
    "    \"ENC_CH\":             ch,\n",
    "    \"ENC_LEN\":            enc_len,\n",
    "    \"ENC_DIM\":            enc_dim,\n",
    "    \"KERNEL_SIZE\":        3,\n",
    "    \"KERNEL_STRIDE\":      1,\n",
    "    \"KERNEL_SIZE_SEQ\":    3,\n",
    "    \"POOL_SIZE\":          2,\n",
    "    \"POOL_STRIDE\":        2,\n",
    "    \"NUM_BIRDS\":          24,\n",
    "    \"N_FOLDS\":            5,\n",
    "    \"BATCH_NUM\":          30,\n",
    "    \"VALID_BATCH_NUM\":    30,\n",
    "    \"EPOCH_NUM\":          100,\n",
    "    \"DROPOUT\":            0.35,\n",
    "    \"lr\":                 0.001,\n",
    "    \"eta_min\":            1e-5,\n",
    "    \"t_max\":              10,\n",
    "    \"TEST_SIZE\":          0.2,\n",
    "    \"MIXUP\":              0.0,\n",
    "    \"MIXUP_PROB\":         -1.0,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "set_seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "\n",
    "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0.:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    # lam = max(lam, 1 - lam)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index,:]\n",
    "    # mixed_y = lam * y + (1 - lam) * y[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    # return mixed_x, mixed_y\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# def mixup_criterion(y_a, y_b, lam):\n",
    "#     return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.RandomCrop((config.mel, config.CLIP_LEN)),\n",
    "    # transforms.ToTensor()\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    # transforms.CenterCrop((config.mel, config.CLIP_LEN)),\n",
    "    # transforms.ToTensor()\n",
    "])\n",
    "label_transform = transforms.Compose([\n",
    "    # transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1216\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      id  species_id recording_id    t_min   t_maxs      offs  0  1  2  3  4  \\\n584  584           8    77299bde7   5.7227   9.8453  274689.6  0  0  0  0  0   \n585  585          21    77299bde7  42.3787  43.4720  427521.6  0  0  0  0  0   \n\n     5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  \n584  0  0  0  1  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  \n585  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>species_id</th>\n      <th>recording_id</th>\n      <th>t_min</th>\n      <th>t_maxs</th>\n      <th>offs</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>584</th>\n      <td>584</td>\n      <td>8</td>\n      <td>77299bde7</td>\n      <td>5.7227</td>\n      <td>9.8453</td>\n      <td>274689.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>585</th>\n      <td>585</td>\n      <td>21</td>\n      <td>77299bde7</td>\n      <td>42.3787</td>\n      <td>43.4720</td>\n      <td>427521.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Data load\n",
    "df_train_tp = pd.read_csv(config.TRAIN_TP_CSV)\n",
    "\n",
    "# add column per birds and flogs\n",
    "for col in range(24):\n",
    "    df_train_tp[col] = 0\n",
    "\n",
    "# one-hot encoding\n",
    "for index, row in df_train_tp.iterrows():\n",
    "    specId = row[\"species_id\"]\n",
    "    for col in range(24):\n",
    "        if int(specId) == col:\n",
    "            df_train_tp.iloc[index, df_train_tp.columns.get_loc(col)] = 1\n",
    "\n",
    "# grouping\n",
    "# df_train_tp = df_train_tp.groupby(\"recording_id\", as_index=False).max()\n",
    "\n",
    "# check\n",
    "print(len(df_train_tp))\n",
    "display(df_train_tp[df_train_tp[\"recording_id\"] == \"77299bde7\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_train_tp[df_train_tp[\"recording_id\"] == \"00b404881\"].head())\n",
    "# df_train_tp_grouped = df_train_tp.groupby([\"species_id\", \"recording_id\"], as_index=False).max()\n",
    "# display(df_train_tp_grouped[df_train_tp_grouped[\"recording_id\"] == \"00b404881\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "id 584\nspecid 8\nrecid 77299bde7\nlabel [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\nlabel shape (24,)\nid len 1216\noffset 274689.6\noffset 5.7227\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ids = []\n",
    "specIds = []\n",
    "record_ids = []\n",
    "labels = []\n",
    "offsets = []\n",
    "for index, row in df_train_tp.iterrows():\n",
    "    ids.append(row.values[0])\n",
    "    specIds.append(row.values[1])\n",
    "    record_ids.append(row.values[2])\n",
    "    labels.append(row.values[6:30])\n",
    "    offsets.append(row.values[5])\n",
    "\n",
    "labels = np.array(labels).astype(float)\n",
    "\n",
    "print('id', ids[584])\n",
    "print('specid', specIds[584])\n",
    "print('recid', record_ids[584])\n",
    "print('label', labels[584])\n",
    "print('label shape', labels[584].shape)\n",
    "print('id len', len(ids))\n",
    "print('offset', offsets[584])\n",
    "print('offset', offsets[584] / params.sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RainforestDatasets(torch.utils.data.Dataset):\n",
    "#     def __init__(self, _transform, train = True):\n",
    "#         self.transform = _transform\n",
    "#         self.train = train\n",
    "\n",
    "#         # data load\n",
    "#         self.labelset = labels\n",
    "#         self.dataset = []\n",
    "#         for id in ids:\n",
    "#             # read npy\n",
    "#             melspec = torch.load(os.path.join(config.TRAIN_AUDIO_ROOT, str(id) + \".pt\")) # (dim, seq_len)\n",
    "#             # melspec = torch.from_numpy(melspec)\n",
    "#             # melspec = melspec.unsqueeze(0) # add channel for first convolution\n",
    "#             # melspec = melspec[np.newaxis, :, :] # add channel for first convolution\n",
    "#             self.dataset.append(melspec)\n",
    "\n",
    "#         self.dataset = np.array(self.dataset).astype(float)\n",
    "#         self.datanum = len(self.dataset)\n",
    "        \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.datanum\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # get data\n",
    "#         out_label = self.labelset[idx]\n",
    "#         out_data = self.dataset[idx]\n",
    "\n",
    "#         # to tensor\n",
    "#         out_label = torch.from_numpy(out_label).float()\n",
    "#         # out_data = torch.from_numpy(out_data).float()\n",
    "        \n",
    "#         # transform label\n",
    "#         # out_data = self.transform(out_data)\n",
    "#         # out_label = label_transform(out_label)\n",
    "\n",
    "#         # out_data = out_data.transpose(0, 1) # (dim, seq_len) => (seq_len, dim)\n",
    "#         # out_data = out_data.unsqueeze(0) # add channel for first convolution (seq_len, dim) => (c, seq_len, dim)\n",
    "#         # out_data = torch.stack([out_data, out_data, out_data]) # add channel for first convolution (seq_len, dim) => (c, seq_len, dim)\n",
    "#         # print(type(out_data))\n",
    "#         # print(type(np.array(out_label)))\n",
    "#         # print(out_data.shape)\n",
    "\n",
    "#         # # encoding on cpu(Its important for reduce usage of gpu memory)\n",
    "#         # out_data = out_data.unsqueeze(0) # add fake batch\n",
    "#         # out_data = model_efn.extract_features(out_data)\n",
    "#         # out_data = out_data[0]\n",
    "\n",
    "#         return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: idx, out: batch(valid), label(s)\n",
    "class RainforestTrainDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = labels\n",
    "        self.ids = ids     \n",
    "        self.datanum = len(labels)   \n",
    "        self.record_ids = record_ids\n",
    "        self.offsets = offsets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get data\n",
    "        out_label = self.labels[idx]\n",
    "\n",
    "        # random crop\n",
    "        randomCropOffset = int((int(np.random.rand() * self.offsets[idx])))\n",
    "\n",
    "        # load wav\n",
    "        wavnp = np.load(Path('../input//rfcx-species-audio-detection/train_mel/' + str(self.ids[idx]) + '.npy'))\n",
    "        if randomCropOffset >= 0:\n",
    "            \n",
    "            wavnp = wavnp[0 + randomCropOffset: (10 * params.sr) + randomCropOffset]\n",
    "        else:\n",
    "            wavnp = wavnp[len(wavnp) - (10 * params.sr) + randomCropOffset : len(wavnp) + randomCropOffset]\n",
    "        wavnp = wav2mel(wavnp) # 10s clipping\n",
    "\n",
    "        # dim, seq_len => seq_len, dim\n",
    "        wavnp = wavnp.T\n",
    "\n",
    "        # add channel\n",
    "        wavnp = np.stack([wavnp, wavnp, wavnp])\n",
    "\n",
    "        # to Tensor\n",
    "        wavTensor = torch.from_numpy(wavnp)\n",
    "\n",
    "        return wavTensor, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in: idx, out: batch(valid), label(s)\n",
    "class RainforestValidDatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.labels = labels\n",
    "        self.ids = ids     \n",
    "        self.datanum = len(labels)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get data\n",
    "        out_label = self.labels[idx]\n",
    "\n",
    "        outdatas = []\n",
    "\n",
    "        #TODO: 11 is magic number, due to change it to conf\n",
    "        for i in range(11):\n",
    "            # if i % 5 == 0 or i == 50:\n",
    "            # load melspec(dim, seq_len)\n",
    "            melspec =  np.load(os.path.join(config.VALID_AUDIO_ROOT, str(self.ids[idx]) + \"_\" + str(i) + \".npy\"))\n",
    "            # add channel\n",
    "            melspec = np.stack([melspec.T, melspec.T, melspec.T])\n",
    "            outdatas.append(melspec)\n",
    "\n",
    "        # list 2 tochtensor(batch, channel, seq_len, dim)\n",
    "        outdatas = torch.from_numpy(np.array(outdatas))\n",
    "\n",
    "        return outdatas, out_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ds train\n",
    "# import librosa.display\n",
    "# ds = RainforestDatasets(train_transform)\n",
    "# loader = DataLoader(ds)\n",
    "\n",
    "# # check aug\n",
    "# for x, y in loader:\n",
    "#     a = 1\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# ax.set(title='train random crop')\n",
    "# img = librosa.display.specshow(\n",
    "#     x.numpy()[0][0].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "\n",
    "\n",
    "# ds = RainforestDatasets(valid_transform)\n",
    "# loader = DataLoader(ds)\n",
    "\n",
    "# # check aug\n",
    "# for x, y in loader:\n",
    "#     a = 1\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# ax.set(title='validation center crop')\n",
    "# img = librosa.display.specshow(\n",
    "#     x.numpy()[0][0].T, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "\n",
    "# # ax.set(title=f'Mel-frequency spectrogram of {row[\"recording_id\"]}')\n",
    "# # fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "# print(x.numpy()[0][0].T.shape)\n",
    "\n",
    "# melspec = np.load(os.path.join(config.TRAIN_AUDIO_ROOT, str(1215) + \".npy\"))\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# img = librosa.display.specshow(\n",
    "#     melspec, \n",
    "#     sr=48000,\n",
    "#     x_axis='time', \n",
    "#     y_axis='linear', \n",
    "#     ax=ax)\n",
    "# print(melspec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformer\n",
    "# https://arxiv.org/abs/2005.08100\n",
    "class RainforestTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RainforestTransformer, self).__init__()         \n",
    "\n",
    "        self.encoding = model_efn\n",
    "        # self.pointwise = nn.Conv2d(config.ENC_CH, 1, (1, 1))\n",
    "        self.conv = nn.Conv2d(config.ENC_CH, 1, (config.KERNEL_SIZE_SEQ, config.KERNEL_SIZE), stride=config.KERNEL_STRIDE)\n",
    "        self.linear = nn.Linear(int((((((config.ENC_DIM - config.KERNEL_SIZE) / config.KERNEL_STRIDE) + 1) - config.POOL_SIZE) / config.POOL_STRIDE) + 1), config.ENC_DIM)\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "        self.conformerblock = ConformerBlock(\n",
    "            dim = config.ENC_DIM,\n",
    "            dim_head = 64,\n",
    "            heads = 8,\n",
    "            ff_mult = 4,\n",
    "            conv_expansion_factor = 2,\n",
    "            conv_kernel_size = 31,\n",
    "            attn_dropout = config.DROPOUT,\n",
    "            ff_dropout = config.DROPOUT,\n",
    "            conv_dropout = config.DROPOUT\n",
    "        )\n",
    "        self.conformerblock2 = ConformerBlock(\n",
    "            dim = config.ENC_DIM,\n",
    "            dim_head = 64,\n",
    "            heads = 8,\n",
    "            ff_mult = 4,\n",
    "            conv_expansion_factor = 2,\n",
    "            conv_kernel_size = 31,\n",
    "            attn_dropout = config.DROPOUT,\n",
    "            ff_dropout = config.DROPOUT,\n",
    "            conv_dropout = config.DROPOUT\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Linear(1 * int((((((config.ENC_LEN - config.KERNEL_SIZE_SEQ) / config.KERNEL_STRIDE) + 1) -  config.POOL_SIZE) / config.POOL_STRIDE) + 1) * config.ENC_DIM, config.NUM_BIRDS)\n",
    "\n",
    "        # devided by stride\n",
    "    \n",
    "    # x: (b, c, seqlen, dim)\n",
    "    def forward(self, x):\n",
    "        # (b, c, seqlen, dim) => (b, c, seqlen, dim)\n",
    "        x = self.encoding.extract_features(x)\n",
    "        # enc = self.pointwise(enc)\n",
    "\n",
    "        # (b, c, seqlen, dim) <= encoded matrix\n",
    "        # point-wise convokution for convolution channel.\n",
    "        h = F.relu(self.conv(x))\n",
    "        h = F.max_pool2d(h, config.POOL_SIZE, stride=config.POOL_STRIDE)\n",
    "        h = self.linear(h)\n",
    "        h = h.transpose(0, 1)[0] # transpose batch and channel to delet channel dimension\n",
    "        h = self.conformerblock(h)\n",
    "        h = self.conformerblock2(h)\n",
    "        # h = self.conformerblock3(h)\n",
    "        # h = self.conformerblock4(h)\n",
    "        # h = self.conformerblock5(h)\n",
    "        # h = self.conformerblock6(h)\n",
    "        h = h.view(-1, 1 * int((((((config.ENC_LEN - config.KERNEL_SIZE_SEQ) / config.KERNEL_STRIDE) + 1) -  config.POOL_SIZE) / config.POOL_STRIDE) + 1) * config.ENC_DIM)\n",
    "        out = self.decoder(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy = torch.stack([clip, clip, clip]).unsqueeze(0)\n",
    "# model = RainforestTransformer()\n",
    "# y = model(sample.unsqueeze(0))\n",
    "# make_dot(y,params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preserve Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete unusual var\n",
    "del sample\n",
    "# del model_efn\n",
    "# del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5, 3, 10, 2)\n(4, 5, 3, 10, 2)\nloop all divnum\n(4, 3, 10, 2)\n0.0\n(4, 3, 10, 2)\n240.0\n(4, 3, 10, 2)\n0.0\n(4, 3, 10, 2)\n240.0\n(4, 3, 10, 2)\n0.0\n"
     ]
    }
   ],
   "source": [
    "allclip = np.stack([\n",
    "    np.zeros([3,10,2]),\n",
    "    np.ones([3,10,2]),\n",
    "    np.zeros([3,10,2]),\n",
    "    np.ones([3,10,2]),\n",
    "    np.zeros([3,10,2]),\n",
    "])\n",
    "# div, c, seq_len, dim\n",
    "print(allclip.shape)\n",
    "\n",
    "# b, d, c, s, d\n",
    "batch = np.stack([allclip, allclip, allclip, allclip])\n",
    "print(batch.shape)\n",
    "\n",
    "# reshape\n",
    "print('loop all divnum')\n",
    "for i in range(batch.shape[1]):\n",
    "    print(batch[:, i, :, :].shape)\n",
    "    # print(batch[])\n",
    "    print(batch[:, i, :, :].sum())\n",
    "    # 4 * 3 * 10 * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[6 3 7]\n  [4 6 9]\n  [2 6 7]\n  [4 3 7]]\n\n [[7 2 5]\n  [4 1 7]\n  [5 1 4]\n  [0 9 5]]]\n[[7 3 7]\n [4 6 9]\n [5 6 7]\n [4 9 7]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(0, 10, (2,4,3))\n",
    "print(a)\n",
    "\n",
    "print(np.max(a, axis=0))\n",
    "# print(np.max(a, axis=1))\n",
    "\n",
    "# print(np.max(a, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "torch.max(torch.Tensor([0,1,2]), dim=0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED)\n",
    "    # msss = MultilabelStratifiedShuffleSplit(n_splits=config.N_FOLDS, test_size=config.TEST_SIZE, random_state=config.SEED)\n",
    "\n",
    "    # Read dataset\n",
    "    train_datasets = RainforestTrainDatasets()\n",
    "    # valid_datasets = RainforestDatasets(_transform=valid_transform)\n",
    "\n",
    "    # valid dataset dosen't need to transform(already be croped)\n",
    "    valid_datasets = RainforestValidDatasets()\n",
    "\n",
    "    best_epochs = []\n",
    "    best_lwlraps = []\n",
    "\n",
    "    # tensorboard\n",
    "    # if IS_WRITRE_LOG:\n",
    "    #     writer = SummaryWriter(log_dir=\"./logs/\" + EXP_NAME)\n",
    "\n",
    "    # k-fold\n",
    "    # for kfoldidx, (train_index, valid_index) in enumerate(msss.split(labels, labels)):\n",
    "    for kfoldidx, (train_index, valid_index) in enumerate(skf.split(specIds, specIds)):\n",
    "\n",
    "        # model \n",
    "        model = RainforestTransformer()\n",
    "        model.to(device)\n",
    "\n",
    "        # init\n",
    "        best_lwlrap = 0.\n",
    "        best_epoch = 0\n",
    "\n",
    "        if IS_WRITRE_LOG:\n",
    "            run = wandb.init(config=config, project=PROJECT, group=EXP_NAME, reinit=True)\n",
    "            print('wandb init')\n",
    "            wandb.run.name = EXP_NAME + '-fold-' + str(kfoldidx)\n",
    "            wandb.run.save()\n",
    "            wandb.watch(model)\n",
    "\n",
    "        # criterion\n",
    "        print('wandb init2')\n",
    "        criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = Adam(params=model.parameters(), lr=config.lr, amsgrad=False)\n",
    "        # print(optimizer)\n",
    "\n",
    "        # train\n",
    "        train_subset = Subset(train_datasets, train_index)\n",
    "        train_loader = DataLoader(train_subset, batch_size=config.BATCH_NUM, shuffle=True)\n",
    "\n",
    "        # validation\n",
    "        valid_subset = Subset(valid_datasets, valid_index)\n",
    "        valid_loader = DataLoader(valid_subset, batch_size=config.VALID_BATCH_NUM, shuffle=False)\n",
    "\n",
    "        # scheduler\n",
    "        # scheduler = CosineAnnealingLR(optimizer, T_max=config.t_max, eta_min=config.eta_min)\n",
    "        num_train_steps = int(len(train_loader) * config.EPOCH_NUM)\n",
    "        num_warmup_steps = int(0.1 * config.EPOCH_NUM * len(train_loader))\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "\n",
    "        # epoch\n",
    "        mb = master_bar(range(config.EPOCH_NUM))\n",
    "        mb.names = ['avg_loss', 'avg_val_loss', 'lwlrap']\n",
    "\n",
    "        # Epoch\n",
    "        for epoch in mb:\n",
    "\n",
    "            # start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train mode\n",
    "            model.train()\n",
    "\n",
    "            # init loss\n",
    "            avg_loss = 0.\n",
    "\n",
    "            # batch training\n",
    "            train_batch_preds = []\n",
    "            train_batch_labels = []\n",
    "            for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "\n",
    "                # dice\n",
    "                dice = np.random.rand(1)\n",
    "                if dice < config.MIXUP_PROB:\n",
    "                    # mixup\n",
    "                    x_batch, y_batch, y_batch_b, lam = mixup_data(x_batch, y_batch, alpha=config.MIXUP, use_cuda=True)\n",
    "\n",
    "                # forward\n",
    "                preds = model(x_batch.to(device))\n",
    "\n",
    "                if dice < config.MIXUP_PROB:\n",
    "                    loss = mixup_criterion(criterion, preds, y_batch.to(device), y_batch_b.to(device), lam)\n",
    "                else:\n",
    "                    loss = criterion(preds, y_batch.to(device)) # It dosen't need Sigmoid, because BCE includes sigmoid function.\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "                del loss\n",
    "\n",
    "                # add preds\n",
    "                train_batch_preds.extend(torch.sigmoid(preds).detach().cpu().numpy().tolist())\n",
    "                train_batch_labels.extend(y_batch.detach().cpu().numpy().tolist())\n",
    "\n",
    "            # calc score\n",
    "            score, weight = calculate_per_class_lwlrap(np.array(train_batch_labels), np.array(train_batch_preds))\n",
    "            train_lwlrap = (score * weight).sum()\n",
    "\n",
    "            # last_preds =  np.array(train_batch_preds)\n",
    "            # last_labels = np.array(train_batch_labels)\n",
    "\n",
    "            # validation mode\n",
    "            model.eval()\n",
    "            valid_batch_preds = []\n",
    "            valid_batch_labels = []\n",
    "            # valid_preds = np.zeros((len(valid_index), config.NUM_BIRDS))\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            # validation\n",
    "            # for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "            for i, (x_batch, y_batch) in tqdm(enumerate(valid_loader)):\n",
    "                # !!!caution!!!\n",
    "                # x_batch's shape (batch, devide length(i.e. 51), channel, seq_len, dim)\n",
    "                # extract column\n",
    "\n",
    "                wholeclip_preds = []\n",
    "            \n",
    "                for divnum in range(x_batch.shape[1]):\n",
    "                    x_batch_divided = x_batch[:, divnum, :, :]\n",
    "                    preds = model(x_batch_divided.to(device)).detach() # (batch, species_id)\n",
    "                    wholeclip_preds.append(preds.cpu().numpy().tolist()) # (divnum, batch, species_id)\n",
    "\n",
    "                # get max via divnum\n",
    "                # (batch, preds_dimention)\n",
    "                preds = torch.max(torch.from_numpy(np.array(wholeclip_preds)).float(), dim=0).values\n",
    "\n",
    "                # preds = model(x_batch.to(device)).detach()\n",
    "                loss = criterion(preds.to(device), y_batch.to(device))\n",
    "\n",
    "                preds = torch.sigmoid(preds)\n",
    "                # valid_preds[i * config.VALID_BATCH_NUM: (i+1) * config.VALID_BATCH_NUM] = preds.cpu().numpy()\n",
    "                avg_val_loss += loss.item() / len(valid_loader)\n",
    "\n",
    "                valid_batch_preds.extend(preds.detach().cpu().numpy().tolist())\n",
    "                valid_batch_labels.extend(y_batch.detach().cpu().tolist())\n",
    "\n",
    "            # calc score\n",
    "            # score, weight = calculate_per_class_lwlrap(labels[valid_index], valid_preds)\n",
    "            score, weight = calculate_per_class_lwlrap(np.array(valid_batch_labels), np.array(valid_batch_preds))\n",
    "            lwlrap = (score * weight).sum()\n",
    "\n",
    "            # update lr\n",
    "            scheduler.step()\n",
    "\n",
    "            # tensorboard\n",
    "            if IS_WRITRE_LOG:\n",
    "                # tensorboard\n",
    "                # writer.add_scalar(\"train_loss/fold-\" + str(kfoldidx), avg_loss, epoch)\n",
    "                # writer.add_scalar(\"valid_loss/fold-\" + str(kfoldidx), avg_val_loss, epoch)\n",
    "                # writer.add_scalar(\"train_lwlrap/fold-\" + str(kfoldidx), train_lwlrap, epoch)\n",
    "                # writer.add_scalar(\"valid_lwlrap/fold-\" + str(kfoldidx), lwlrap, epoch)\n",
    "\n",
    "                wandb.log({\n",
    "                    'loss/train': avg_loss,\n",
    "                    'lwlrap/train': train_lwlrap,\n",
    "                    'loss/validation': avg_val_loss,\n",
    "                    'lwlrap/validation': lwlrap,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} train_lwlrap: {train_lwlrap:.6f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n",
    "        \n",
    "            if lwlrap > best_lwlrap and epoch > 50:\n",
    "                best_epoch = epoch + 1\n",
    "                best_lwlrap = lwlrap\n",
    "                # torch.save(model.state_dict(), 'weight_best_' + str(EXP_NUM) + '_fold' + str(kfoldidx) +'.pt')\n",
    "                torch.save(model.state_dict(), 'weight_best_fold' + str(kfoldidx) +'.pt')\n",
    "            \n",
    "        best_epochs.append(best_epoch)\n",
    "        best_lwlraps.append(best_lwlrap)\n",
    "\n",
    "        # return last_preds, last_labels\n",
    "\n",
    "        if IS_WRITRE_LOG:\n",
    "            run.finish()\n",
    "\n",
    "    # if IS_WRITRE_LOG:\n",
    "    #     writer.close()\n",
    "    \n",
    "    return {\n",
    "        'best_epoch': best_epochs,\n",
    "        'best_lwlrap': best_lwlraps,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "wandb: Currently logged in as: tatsuya-takahashi (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "result = train()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "models = []\n",
    "for fold in range(config.N_FOLDS):\n",
    "    # load network\n",
    "    model = RainforestTransformer()\n",
    "    # torch.save(model.state_dict(), 'weight_best_' + str(EXP_NUM) + '_fold' + str(kfoldidx) +'.pt')\n",
    "    model.load_state_dict(torch.load('weight_best_fold' + str(fold) +'.pt'))\n",
    "    # print('weight_best_' + str(EXP_NUM) + '_fold' + str(fold) +'.pt')\n",
    "    # model.load_state_dict(torch.load('weight_best_' + str(EXP_NUM) + '_fold' + str(fold) +'.pt'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_exp_13_BackbornEFN4_800.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680f1d7ec82f41dfa5f3888ddbf77642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1992.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted for 100 of 1993 files\n",
      "Predicted for 200 of 1993 files\n",
      "Predicted for 300 of 1993 files\n",
      "Predicted for 400 of 1993 files\n",
      "Predicted for 500 of 1993 files\n",
      "Predicted for 600 of 1993 files\n",
      "Predicted for 700 of 1993 files\n",
      "Predicted for 800 of 1993 files\n",
      "Predicted for 900 of 1993 files\n",
      "Predicted for 1000 of 1993 files\n",
      "Predicted for 1100 of 1993 files\n",
      "Predicted for 1200 of 1993 files\n",
      "Predicted for 1300 of 1993 files\n",
      "Predicted for 1400 of 1993 files\n",
      "Predicted for 1500 of 1993 files\n",
      "Predicted for 1600 of 1993 files\n",
      "Predicted for 1700 of 1993 files\n",
      "Predicted for 1800 of 1993 files\n",
      "Predicted for 1900 of 1993 files\n",
      "\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# write submission\n",
    "with open('submission_' + EXP_NAME + '_800.csv', 'w', newline='') as csvfile:\n",
    "    print('submission_' + EXP_NAME + '_800.csv')\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11',\n",
    "                               's12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23'])\n",
    "    \n",
    "    test_files = os.listdir(config.TEST_AUDIO_ROOT)\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in tqdm(range(0, len(test_files))):\n",
    "    # for i in range(0, 1):\n",
    "        # read data\n",
    "        # X_test = torch.from_numpy(np.load(os.path.join(config.TEST_AUDIO_ROOT, test_files[i])))\n",
    "\n",
    "        # (dim, seq_len)        \n",
    "        X_test = np.load(os.path.join(config.TEST_AUDIO_ROOT, test_files[i]))\n",
    "        X_test_batch = []\n",
    "\n",
    "        # duplicate time\n",
    "        duplicate_length = 800\n",
    "\n",
    "        # non duplicate length\n",
    "        hop_length = config.CLIP_LEN - duplicate_length\n",
    "\n",
    "        # muptiply number\n",
    "        dev_num = math.floor((X_test.shape[1] - duplicate_length) / hop_length)\n",
    "\n",
    "        # Cutting!\n",
    "        for idx in range(dev_num):\n",
    "            X_test_clip = X_test[:, idx * hop_length : (idx * hop_length) + config.CLIP_LEN]\n",
    "            X_test_clip = X_test_clip.T\n",
    "            # X_test_clip = X_test_clip[np.newaxis, :, :] # add fake channel\n",
    "            X_test_clip = np.stack([X_test_clip, X_test_clip, X_test_clip]) # expand to channel\n",
    "            X_test_batch.append(X_test_clip.tolist())\n",
    "\n",
    "        if not (hop_length * dev_num) + duplicate_length == X_test.shape[1]:\n",
    "            # last cutting\n",
    "            X_test_clip = X_test[:, X_test.shape[1] - config.CLIP_LEN : X_test.shape[1]]\n",
    "            X_test_clip = X_test_clip.T\n",
    "            # X_test_clip = X_test_clip[np.newaxis, :, :] # add fake channel\n",
    "            X_test_clip = np.stack([X_test_clip, X_test_clip, X_test_clip]) # expand to channel\n",
    "            X_test_batch.append(X_test_clip.tolist())\n",
    "\n",
    "        # to_tensor\n",
    "        X_test_batch = torch.from_numpy(np.array(X_test_batch)).float()\n",
    "        X_test_batch = X_test_batch.to(device)\n",
    "\n",
    "        # predict\n",
    "        output_list = []\n",
    "        for m in models:\n",
    "            outputs = []\n",
    "            for x_b in X_test_batch:\n",
    "                output = m(torch.stack([x_b]))\n",
    "                outputs.append(output[0].cpu().detach().numpy().tolist())\n",
    "            maxed_output = torch.max(torch.from_numpy(np.array(outputs)).float(), dim=0) # max about batch clips\n",
    "            maxed_output = maxed_output.values.cpu().detach()\n",
    "            output_list.append(maxed_output)\n",
    "        avg_maxed_output = torch.mean(torch.stack(output_list), dim=0)\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "#         for out in maxed_output:\n",
    "        for out in avg_maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp_13_BackbornEFN4'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4., 5., 6.]),\n",
       "indices=tensor([1, 1, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.from_numpy(np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6]\n",
    "])).float(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}